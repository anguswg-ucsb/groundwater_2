---
title: "USGS_NWIS_dtw_17"
author: "Lydia Bleifuss"
date: "4/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Packages}
library(dataRetrieval)

#Other 

library(tidyverse)
library(ggplot2)
library(leaflet)
library(janitor)
library(here)
library(readxl)
library(kableExtra)
library(skimr)
library(naniar)
library(VIM)
library(ggfortify)
library(lubridate)
library(tsibble)
library(dplyr)

#Spatial Clipping 
library(USAboundaries) # Download shape files
library(USAboundariesData)
library(sf) 

```

###Pulling in ALL groundwater levels from USGS by State:

**Please reference WY at the bottom of this .RMD for initial process ensuring I was getting all sites and data, and reference AZ for USGS pull annotations**

1. PULL USGS Data
2. JOIN site and dtw dataframes and remove any rows with missing spatial info (lat/long) or depth to water measurements
3. CLIP to state boundaries to ensure all wells are w/in state
5. CLEAN, remove any rows w/ missing dates and add calculated columns, and keep unique wells (Lydia, might need to keep all measurments if crossover is weird and need to compare measurments...for now, keep only unique wells for simplicity)
6. Take the cleaned dataframe over to State_National_Join folder and join with State data

THEN

- Conduct Crossover analysis
- Negative Analysis
- Export: Unique wells with info calculated, no spear


####**Arizona** 
```{r AZ PULL}
#Sites
az_sites <- readNWISdata(stateCd="Arizona", #for Arizona
                         service="site", #pull all sites held w/in USGS/NWIS
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% #rename columns so they are universal across all pulls
  filter(site_tp_cd == "GW") %>% #only keep sites associated with groundwater
  dplyr::select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% #only keep relevent columns 
distinct(site_no, .keep_all = TRUE) #only keeping distinct sites to ensure no repeats

#Dtw
az_dtw <- readNWISdata(stateCd="Arizona", #for Arizona
                       service="gwlevels", #pull all groundwater levels
                        startDT="1850-01-01", #start as early as possible
                       endDT="2020-04-22") %>% #start as current as possible
  renameNWISColumns() %>%  #rename columns so they are universal across all pulls
  dplyr::select(agency_cd, site_no, lev_dt, lev_va) #only keep relevent columns 

```


```{r AZ JOIN}
#Join
az_nwis_join <- left_join(az_dtw, az_sites, by = "site_no") %>% #join site information to each dtw site row 
  dplyr::select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in Arizona Clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(az_nwis_join$site_id)) #checking in on how many sites there are left 
#35794
```



```{r AZ CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

az_shp <- us_states(resolution = "high", states = "AZ") %>%  #read in AZ shp file
  st_transform(crs = 4269) #set CRS for NAD83

st_crs(az_shp) #was 4326, after st_transform is 4269 #NEEDED TO REPROJECT ALL .SHP (do not have to manually assign crs because crs is defined)



az_nwis_spatial <- az_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"),
           crs = 4269)

#st_crs(az_nwis_spatial) #4269, this works to set the crs as NAD83 because USGS NWIS is in NAD83


az_clip <- st_intersection(az_shp, az_nwis_spatial) %>% # This filters for points inside of the az shape file
  select(site_id)


length(unique(az_clip$site_id)) #checking in on how many sites there are left 
#35780 (so 14 wells were taken out of the df that were outside AZ bounds)


#CHECK in map
az_map <- ggplot() +
  geom_sf(data = az_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = az_shp,
          fill = NA,
          colour = "black")
az_map

#Create df with only wells inside the state boundary 
az_nwis_clip <- inner_join(az_nwis_join, az_clip, by = "site_id")

length(unique(az_nwis_clip$site_id)) #checking in on how many sites there are left 
#35780 (Great! Kept out the 14 wells in the new df, which now has sticky geom column)

```

```{r AZ CLEAN}
#Clean

az_nwis_clean <- az_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

az_all <- az_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

az_unique_sites <- az_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(az_unique_sites, here::here("National","CSV Export", "az_unique_sites.csv"))

write.csv(az_all, here::here("National", "CSV Export", "az_all.csv"))

```


```{r AZ Criteria?}

#Criteria: > =10 measurements, >= 5 distinct years, 1 pt after 2010, 1 pt before 1980 
 
# az_spear <- az_nwis_all %>% #creating new df to calculate Spearman's Rank 
#   group_by(site_id) %>% #by well 
#   dplyr::summarise(spear_rank = cor(dec_date, dtw, 
#                    method='spearman'))
# 
# az_nwis_spear <- inner_join(az_spear, az_nwis_all, by = "site_id") %>% #adding in df with spear ranks and simplifying w/ criteria
#     filter(measurement_dist >= 10) %>% #only kept wells with >= 10 measurments
#   filter(year_dist >=5) %>% #only kept wells with >=5 distinct years
#   filter(date_max >= 2010) %>% #identifying wells that have measurements at or beyond 2010
#   filter(date_min <= 1980) %>% #identifying wells that have measurements at or before 1980
#   distinct(site_id, .keep_all = TRUE) %>%  #only keeping unique rows (don't need all dtw measurements for the purpose of this dataset)
#   select(agency_cd, site_id, lat, long, date_min, date_max, measurement_dist, year_dist, spear_rank) #%>% 
#   #drop_na(lat) #no missing lat, long
# 
# length(unique(az_nwis_spear$site_id))

```


```{r AZ NEG}
#Negative Analysis
# 
# az_neg_nwis_wells <- inner_join(az_spear, az_nwis_all, by = "site_id") %>% #adding in df with spear ranks and simplifying w/ criteria
#     filter(measurement_dist >= 10) %>% #only kept wells with >= 10 measurments
#   filter(year_dist >=5) %>% #only kept wells with >=5 distinct years
#   filter(date_max >= 2010) %>% #identifying wells that have measurements at or beyond 2010
#   filter(date_min <= 1980) %>% #identifying wells that have measurements at or before 1980
#   filter(dtw < 0) %>% #79 negative Measurements
#   select(site_id) %>% #only keeping these well ids so I can make a df will all well measurements to evaluate neg values in context 
#   distinct(site_id, .keep_all = TRUE) 
# 
# az_neg_nwis <- inner_join(az_neg_nwis_wells, az_nwis_all, by = "site_id") %>% 
#   select(site_id, date, dtw, date_min, date_max)
# 
# az_split_df <- split(az_neg_nwis_wells, az_neg_nwis_wells$site_id) #Can click on each well icon in list to assess well individually and sort by date for greater eficiency 
#   

##Analysis results in negative dtw tracking Google Doc sheet 
```


####**California** 
```{r CA PULL}
#Sites
ca_sites <- readNWISdata(stateCd="California", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
ca_dtw <- readNWISdata(stateCd="California", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)
```


```{r CA JOIN}
#Join
ca_nwis_join <- left_join(ca_dtw, ca_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state Clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(ca_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r CA CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

ca_shp <- us_states(resolution = "high", states = "CA") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

ca_nwis_spatial <- ca_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

ca_clip <- st_intersection(ca_shp, ca_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(ca_clip$site_id)) #checking in on how many sites there are left 
#42776 (so ~10 wells were taken out of the df that were outside bounds)


#CHECK in map
ca_map <- ggplot() +
  geom_sf(data = ca_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = ca_shp,
          fill = NA,
          colour = "black")
#ca_map

#Create df with only wells inside the state boundary 
ca_nwis_clip <- inner_join(ca_nwis_join, ca_clip, by = "site_id")

length(unique(ca_nwis_clip$site_id)) #checking in on how many sites there are left 
#42776 (Great!)

```


```{r CA CLEAN}
#Clean

ca_nwis_clean <- ca_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

ca_all <- ca_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

ca_unique_sites <- ca_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(ca_unique_sites, here::here("National","CSV Export", "ca_unique_sites.csv"))

write.csv(ca_all, here::here("National", "CSV Export", "ca_all.csv"))

```


####**Colorado** 
```{r CO PULL}
#Sites
co_sites <- readNWISdata(stateCd="Colorado", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
co_dtw <- readNWISdata(stateCd="Colorado", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

```


```{r CO JOIN}
#Join
co_nwis_join <- left_join(co_dtw, co_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state chunk after formatting dates in workable format w/ lubridate ets 

length(unique(co_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r CO CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

co_shp <- us_states(resolution = "high", states = "CO") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

co_nwis_spatial <- co_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

co_clip <- st_intersection(co_shp, co_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(co_clip$site_id)) #checking in on how many sites there are left 
#20856 (so 1 well was taken out of the df that were outside bounds)


#CHECK in map
co_map <- ggplot() +
  geom_sf(data = co_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = co_shp,
          fill = NA,
          colour = "black")
#co_map

#Create df with only wells inside the state boundary 
co_nwis_clip <- inner_join(co_nwis_join, co_clip, by = "site_id")

length(unique(co_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r CO CLEAN}
#Clean

co_nwis_clean <- co_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

co_all <- co_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

co_unique_sites <- co_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(co_unique_sites, here::here("National","CSV Export", "co_unique_sites.csv"))

write.csv(co_all, here::here("National", "CSV Export", "co_all.csv"))

```


####**Idaho** 
```{r ID PULL}
#Sites
id_sites <- readNWISdata(stateCd="Idaho", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
id_dtw <- readNWISdata(stateCd="Idaho", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
id_nwis_join <- left_join(id_dtw, id_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(id_nwis_join$site_no))
```


```{r ID JOIN}
#Join
id_nwis_join <- left_join(id_dtw, id_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(id_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r ID CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

id_shp <- us_states(resolution = "high", states = "ID") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

id_nwis_spatial <- id_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

id_clip <- st_intersection(id_shp, id_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(id_clip$site_id)) #checking in on how many sites there are left 
#18973 (so 1 well was taken out of the df that were outside bounds)


#CHECK in map
id_map <- ggplot() +
  geom_sf(data = id_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = id_shp,
          fill = NA,
          colour = "black")
#id_map

#Create df with only wells inside the state boundary 
id_nwis_clip <- inner_join(id_nwis_join, id_clip, by = "site_id")

length(unique(id_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r ID CLEAN}
#Clean

id_nwis_clean <- id_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

id_all <- id_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

id_unique_sites <- id_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(id_unique_sites, here::here("National","CSV Export", "id_unique_sites.csv"))

write.csv(id_all, here::here("National", "CSV Export", "id_all.csv"))

```


####**Kansas** 
```{r KS PULL}
#Sites
ks_sites <- readNWISdata(stateCd="Kansas", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
ks_dtw <- readNWISdata(stateCd="Kansas", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
ks_nwis_join <- left_join(ks_dtw, ks_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(ka_nwis_join$site_no))
```


```{r KS JOIN}
#Join
ks_nwis_join <- left_join(ks_dtw, ks_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(ks_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r KS CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

ks_shp <- us_states(resolution = "high", states = "KS") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

ks_nwis_spatial <- ks_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

ks_clip <- st_intersection(ks_shp, ks_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(ks_clip$site_id)) #checking in on how many sites there are left 
#26654 (so ~15 well was taken out of the df that were outside bounds)


#CHECK in map
ks_map <- ggplot() +
  geom_sf(data = ks_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = ks_shp,
          fill = NA,
          colour = "black")
#ks_map

#Create df with only wells inside the state boundary 
ks_nwis_clip <- inner_join(ks_nwis_join, ks_clip, by = "site_id")

length(unique(ks_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r KS CLEAN}
#Clean

ks_nwis_clean <- ks_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

ks_all <- ks_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

ks_unique_sites <- ks_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(ks_unique_sites, here::here("National","CSV Export", "ks_unique_sites.csv"))

write.csv(ks_all, here::here("National", "CSV Export", "ks_all.csv"))

```


####**Montana** 
```{r MT PULL}
#Sites
mt_sites <- readNWISdata(stateCd="Montana", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
mt_dtw <- readNWISdata(stateCd="Montana", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
mt_nwis_join <- left_join(mt_dtw, mt_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(mt_nwis_join$site_no))
```

```{r MT JOIN}
#Join
mt_nwis_join <- left_join(mt_dtw, mt_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(mt_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r MT CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

mt_shp <- us_states(resolution = "high", states = "MT") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

mt_nwis_spatial <- mt_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

mt_clip <- st_intersection(mt_shp, mt_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(mt_clip$site_id)) #checking in on how many sites there are left 
#18618 (so 1 well was taken out of the df that were outside bounds)


#CHECK in map
mt_map <- ggplot() +
  geom_sf(data = mt_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = mt_shp,
          fill = NA,
          colour = "black")
mt_map

#Create df with only wells inside the state boundary 
mt_nwis_clip <- inner_join(mt_nwis_join, mt_clip, by = "site_id")

length(unique(mt_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r MT CLEAN}
#Clean

mt_nwis_clean <- mt_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

mt_all <- mt_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

mt_unique_sites <- mt_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(mt_unique_sites, here::here("National","CSV Export", "mt_unique_sites.csv"))

write.csv(mt_all, here::here("National", "CSV Export", "mt_all.csv"))

```


####**Nebraska** 
```{r NE PULL}
#Sites
ne_sites <- readNWISdata(stateCd="Nebraska", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
ne_dtw <- readNWISdata(stateCd="Nebraska", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
ne_nwis_join <- left_join(ne_dtw, ne_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(ne_nwis_join$site_no))
```

```{r NE JOIN}
#Join
ne_nwis_join <- left_join(ne_dtw, ne_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(ne_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r NE CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

ne_shp <- us_states(resolution = "high", states = "NE") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

ne_nwis_spatial <- ne_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

ne_clip <- st_intersection(ne_shp, ne_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(ne_clip$site_id)) #checking in on how many sites there are left 
#18618 (so 1 well was taken out of the df that were outside bounds)


#CHECK in map
ne_map <- ggplot() +
  geom_sf(data = ne_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = ne_shp,
          fill = NA,
          colour = "black")
ne_map

#Create df with only wells inside the state boundary 
ne_nwis_clip <- inner_join(ne_nwis_join, ne_clip, by = "site_id")

length(unique(ne_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r NE CLEAN}
#Clean

ne_nwis_clean <- ne_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

ne_all <- ne_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

ne_unique_sites <- ne_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(ne_unique_sites, here::here("National","CSV Export", "ne_unique_sites.csv"))

write.csv(ne_all, here::here("National", "CSV Export", "ne_all.csv"))

```


####**Nevada** 
```{r NV PULL JOIN}
#Sites
nv_sites <- readNWISdata(stateCd="Nevada", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
nv_dtw <- readNWISdata(stateCd="Nevada", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
nv_nwis_join <- left_join(nv_dtw, nv_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(nv_nwis_join$site_no))
```

```{r NV JOIN}
#Join
nv_nwis_join <- left_join(nv_dtw, nv_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(nv_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r NV CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

nv_shp <- us_states(resolution = "high", states = "NV") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

nv_nwis_spatial <- nv_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

nv_clip <- st_intersection(nv_shp, nv_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(nv_clip$site_id)) #checking in on how many sites there are left 
#11623 (so ~10 wells were taken out of the df that were outside bounds)


#CHECK in map
nv_map <- ggplot() +
  geom_sf(data = nv_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = nv_shp,
          fill = NA,
          colour = "black")
nv_map

#Create df with only wells inside the state boundary 
nv_nwis_clip <- inner_join(nv_nwis_join, nv_clip, by = "site_id")

length(unique(nv_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r NV CLEAN}
#Clean

nv_nwis_clean <- nv_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

nv_all <- nv_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

nv_unique_sites <- nv_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(nv_unique_sites, here::here("National","CSV Export", "nv_unique_sites.csv"))

write.csv(nv_all, here::here("National", "CSV Export", "nv_all.csv"))

```

####**New Mexico** 
```{r NM PULL JOIN}
#Sites
nm_sites <- readNWISdata(stateCd="New Mexico", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
nm_dtw <- readNWISdata(stateCd="New Mexico", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
nm_nwis_join <- left_join(nm_dtw, nm_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(nm_nwis_join$site_no))
```


```{r NM JOIN}
#Join
nm_nwis_join <- left_join(nm_dtw, nm_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(nm_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r NM CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

nm_shp <- us_states(resolution = "high", states = "NM") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

nm_nwis_spatial <- nm_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

nm_clip <- st_intersection(nm_shp, nm_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(nm_clip$site_id)) #checking in on how many sites there are left 
#28634 (so ~30 wells were taken out of the df that were outside bounds)


#CHECK in map
nm_map <- ggplot() +
  geom_sf(data = nm_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = nm_shp,
          fill = NA,
          colour = "black")
nm_map

#Create df with only wells inside the state boundary 
nm_nwis_clip <- inner_join(nm_nwis_join, nm_clip, by = "site_id")

length(unique(nm_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r NM CLEAN}
#Clean

nm_nwis_clean <- nm_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

nm_all <- nm_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

nm_unique_sites <- nm_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(nm_unique_sites, here::here("National","CSV Export", "nm_unique_sites.csv"))

write.csv(nm_all, here::here("National", "CSV Export", "nm_all.csv"))

```


####**North Dakota** 
```{r ND PULL}
#Sites
nd_sites <- readNWISdata(stateCd="North Dakota", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
nd_dtw <- readNWISdata(stateCd="North Dakota", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
nd_nwis_join <- left_join(nd_dtw, nd_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(nd_nwis_join$site_no))
```

```{r ND JOIN}
#Join
nd_nwis_join <- left_join(nd_dtw, nd_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(nd_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r ND CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

nd_shp <- us_states(resolution = "high", states = "ND") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

nd_nwis_spatial <- nd_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

nd_clip <- st_intersection(nd_shp, nd_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(nd_clip$site_id)) #checking in on how many sites there are left 
#18463 (so ~5 wells were taken out of the df that were outside bounds)


#CHECK in map
nd_map <- ggplot() +
  geom_sf(data = nd_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = nd_shp,
          fill = NA,
          colour = "black")
nd_map

#Create df with only wells inside the state boundary 
nd_nwis_clip <- inner_join(nd_nwis_join, nd_clip, by = "site_id")

length(unique(nd_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r ND CLEAN}
#Clean

nd_nwis_clean <- nd_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

nd_all <- nd_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

nd_unique_sites <- nd_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(nd_unique_sites, here::here("National","CSV Export", "nd_unique_sites.csv"))

write.csv(nd_all, here::here("National", "CSV Export", "nd_all.csv"))

```


####**Oklahoma** 
```{r OK PULL}
#Sites
ok_sites <- readNWISdata(stateCd="Oklahoma", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
ok_dtw <- readNWISdata(stateCd="Oklahoma", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
ok_nwis_join <- left_join(ok_dtw, ok_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(ok_nwis_join$site_no))
```

```{r OK JOIN}
#Join
ok_nwis_join <- left_join(ok_dtw, ok_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(ok_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r OK CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

ok_shp <- us_states(resolution = "high", states = "OK") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

ok_nwis_spatial <- ok_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

ok_clip <- st_intersection(ok_shp, ok_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(ok_clip$site_id)) #checking in on how many sites there are left 
#20335 (so ~20 wells were taken out of the df that were outside bounds)


#CHECK in map
ok_map <- ggplot() +
  geom_sf(data = ok_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = ok_shp,
          fill = NA,
          colour = "black")
ok_map

#Create df with only wells inside the state boundary 
ok_nwis_clip <- inner_join(ok_nwis_join, ok_clip, by = "site_id")

length(unique(ok_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r OK CLEAN}
#Clean

ok_nwis_clean <- ok_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

ok_all <- ok_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

ok_unique_sites <- ok_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(ok_unique_sites, here::here("National","CSV Export", "ok_unique_sites.csv"))

write.csv(ok_all, here::here("National", "CSV Export", "ok_all.csv"))

```

####**Oregon** 
```{r OR PULL}
#Sites
or_sites <- readNWISdata(stateCd="Oregon", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
or_dtw <- readNWISdata(stateCd="Oregon", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
or_nwis_join <- left_join(or_dtw, or_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(or_nwis_join$site_no))
```

```{r OR JOIN}
#Join
or_nwis_join <- left_join(or_dtw, or_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(or_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r OR CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

or_shp <- us_states(resolution = "high", states = "OR") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

or_nwis_spatial <- or_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

or_clip <- st_intersection(or_shp, or_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(or_clip$site_id)) #checking in on how many sites there are left 
#8600 (so 4 wells were taken out of the df that were outside bounds)


#CHECK in map
or_map <- ggplot() +
  geom_sf(data = or_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = or_shp,
          fill = NA,
          colour = "black")
or_map

#Create df with only wells inside the state boundary 
or_nwis_clip <- inner_join(or_nwis_join, or_clip, by = "site_id")

length(unique(or_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r OR CLEAN}
#Clean

or_nwis_clean <- or_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

or_all <- or_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

or_unique_sites <- or_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(or_unique_sites, here::here("National","CSV Export", "or_unique_sites.csv"))

write.csv(or_all, here::here("National", "CSV Export", "or_all.csv"))

```


####**South Dakota** 
```{r SD PULL}
#Sites
sd_sites <- readNWISdata(stateCd="South Dakota", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
sd_dtw <- readNWISdata(stateCd="South Dakota", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
sd_nwis_join <- left_join(sd_dtw, sd_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(sd_nwis_join$site_no))
```


```{r SD JOIN}
#Join
sd_nwis_join <- left_join(sd_dtw, sd_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(sd_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r SD CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

sd_shp <- us_states(resolution = "high", states = "SD") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

sd_nwis_spatial <- sd_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

sd_clip <- st_intersection(sd_shp, sd_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(sd_clip$site_id)) #checking in on how many sites there are left 
#31373 (so ~10 wells were taken out of the df that were outside bounds)


#CHECK in map
sd_map <- ggplot() +
  geom_sf(data = sd_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = sd_shp,
          fill = NA,
          colour = "black")
sd_map

#Create df with only wells inside the state boundary 
sd_nwis_clip <- inner_join(sd_nwis_join, sd_clip, by = "site_id")

length(unique(sd_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r SD CLEAN}
#Clean

sd_nwis_clean <- sd_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

sd_all <- sd_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

sd_unique_sites <- sd_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(sd_unique_sites, here::here("National","CSV Export", "sd_unique_sites.csv"))

write.csv(sd_all, here::here("National", "CSV Export", "sd_all.csv"))

```

####**Texas** 
```{r TX PULL}
#Sites
tx_sites <- readNWISdata(stateCd="Texas", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
tx_dtw <- readNWISdata(stateCd="Texas", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
tx_nwis_join <- left_join(tx_dtw, tx_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(tx_nwis_join$site_no))
```


```{r TX JOIN}
#Join
tx_nwis_join <- left_join(tx_dtw, tx_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(tx_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r TX CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

tx_shp <- us_states(resolution = "high", states = "TX") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

tx_nwis_spatial <- tx_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

tx_clip <- st_intersection(tx_shp, tx_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(tx_clip$site_id)) #checking in on how many sites there are left 
#19354 (so ~30 wells were taken out of the df that were outside bounds)


#CHECK in map
tx_map <- ggplot() +
  geom_sf(data = tx_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = tx_shp,
          fill = NA,
          colour = "black")
tx_map

#Create df with only wells inside the state boundary 
tx_nwis_clip <- inner_join(tx_nwis_join, tx_clip, by = "site_id")

length(unique(tx_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r TX CLEAN}
#Clean

tx_nwis_clean <- tx_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

tx_all <- tx_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

tx_unique_sites <- tx_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(tx_unique_sites, here::here("National","CSV Export", "tx_unique_sites.csv"))

write.csv(tx_all, here::here("National", "CSV Export", "tx_all.csv"))

```

####**Utah** 
```{r UT PULL}
#Sites
ut_sites <- readNWISdata(stateCd="Utah", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
ut_dtw <- readNWISdata(stateCd="Utah", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
ut_nwis_join <- left_join(ut_dtw, ut_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(ut_nwis_join$site_no))
```


```{r UT JOIN}
#Join
ut_nwis_join <- left_join(ut_dtw, ut_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(ut_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r UT CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

ut_shp <- us_states(resolution = "high", states = "UT") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

ut_nwis_spatial <- ut_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

ut_clip <- st_intersection(ut_shp, ut_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(ut_clip$site_id)) #checking in on how many sites there are left 
#15267 (so ~10 wells were taken out of the df that were outside bounds)


#CHECK in map
ut_map <- ggplot() +
  geom_sf(data = ut_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = ut_shp,
          fill = NA,
          colour = "black")
ut_map

#Create df with only wells inside the state boundary 
ut_nwis_clip <- inner_join(ut_nwis_join, ut_clip, by = "site_id")

length(unique(ut_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r UT CLEAN}
#Clean

ut_nwis_clean <- ut_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

ut_all <- ut_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

ut_unique_sites <- ut_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(ut_unique_sites, here::here("National","CSV Export", "ut_unique_sites.csv"))

write.csv(ut_all, here::here("National", "CSV Export", "ut_all.csv"))

```


####**Washington** 
```{r WA PULL}
#Sites
wa_sites <- readNWISdata(stateCd="Washington", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
wa_dtw <- readNWISdata(stateCd="Washington", 
                       service="gwlevels",
                        startDT="1850-01-01",
                       endDT="2020-04-22") %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join
wa_nwis_join <- left_join(wa_dtw, wa_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(wa_nwis_join$site_no))
```


```{r WA JOIN}
#Join
wa_nwis_join <- left_join(wa_dtw, wa_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(wa_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r WA CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

wa_shp <- us_states(resolution = "high", states = "WA") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

wa_nwis_spatial <- wa_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

wa_clip <- st_intersection(wa_shp, wa_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(wa_clip$site_id)) #checking in on how many sites there are left 
#72830 (so ~250 wells were taken out of the df that were outside bounds)


#CHECK in map
wa_map <- ggplot() +
  geom_sf(data = wa_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = wa_shp,
          fill = NA,
          colour = "black")
wa_map

#Create df with only wells inside the state boundary 
wa_nwis_clip <- inner_join(wa_nwis_join, wa_clip, by = "site_id")

length(unique(wa_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r WA CLEAN}
#Clean

wa_nwis_clean <- wa_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

wa_all <- wa_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

wa_unique_sites <- wa_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(wa_unique_sites, here::here("National","CSV Export", "wa_unique_sites.csv"))

write.csv(wa_all, here::here("National", "CSV Export", "wa_all.csv"))

```


####**Wyoming** 
```{r WY PULL}
#Sites
wy_sites <- readNWISdata(stateCd="Wyoming", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include site_tp_cd
    renameNWISColumns() %>% 
  filter(site_tp_cd == "GW") %>% 
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd) %>% 
distinct(site_no, .keep_all = TRUE) 

#Dtw
wy_dtw <- readNWISdata(stateCd="Wyoming", 
                       service="gwlevels",
                       startDT="1850-01-01",
                       endDT="2020-04-22"
                       ) %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)

#Join 
wy_nwis_join <- left_join(wy_dtw, wy_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(wy_nwis_join$site_no))
```



```{r WY JOIN}
#Join
wy_nwis_join <- left_join(wy_dtw, wy_sites, by = "site_no") %>% #join site information to each dtw site row 
  select(agency_cd.x, site_no, dec_lat_va, dec_long_va, lev_dt, lev_va) %>% 
  dplyr::rename(agency_cd = agency_cd.x, 
                site_id = site_no, 
                dtw = lev_va, 
                lat = dec_lat_va, 
                long = dec_long_va, 
                measure_date = lev_dt) %>% #renaming to have reproducible code below
  drop_na(dtw) %>% #drop any rows with depth to water measurments
  drop_na(lat) #drop any rows that do not have lat/long measurements

#Note: dropping rows w. missing dates in state clean chunk after formatting dates in workable format w/ lubridate ets 

length(unique(wy_nwis_join$site_id)) #checking in on how many sites there are left 

```


```{r WY CLIP}

#Great reference for Coord System vs. CRS
#https://www.nceas.ucsb.edu/sites/default/files/2020-04/OverviewCoordinateReferenceSystems.pdf

wy_shp <- us_states(resolution = "high", states = "WY") %>%  #read in state shp file
  st_transform(crs = 4269) #set CRS for NAD83

wy_nwis_spatial <- wy_nwis_join %>% 
  distinct(site_id, .keep_all = TRUE) %>% #only looking for unique sites so less to process spatially 
  st_as_sf(coords=c("long",
                    "lat"), #creating geometry column with lat and long to clip with state_shp 
           crs=4269) #NAD83 (EPSG:4269) 
      

wy_clip <- st_intersection(wy_shp, wy_nwis_spatial) %>% # This filters for points inside of the state shape file
  select(site_id)


length(unique(wy_clip$site_id)) #checking in on how many sites there are left 
#10083 (so 5 wells were taken out of the df that were outside bounds)


#CHECK in map
wy_map <- ggplot() +
  geom_sf(data = wy_clip,
          colour = "forestgreen", 
           size = 0.1,
           alpha = 0.5)+
  geom_sf(data = wy_shp,
          fill = NA,
          colour = "black")
wy_map

#Create df with only wells inside the state boundary 
wy_nwis_clip <- inner_join(wy_nwis_join, wy_clip, by = "site_id")

length(unique(wy_nwis_clip$site_id)) #checking in on how many sites there are left, same as co_clip, great! 

```


```{r WY CLEAN}
#Clean

wy_nwis_clean <- wy_nwis_clip %>% 
  mutate(date = as.factor(measure_date)) %>% #converting from character to factor to change to date
  mutate(date = as.Date(date, format ="%Y-%m-%d")) %>% #putting date into useable format
  drop_na(date) %>% #some measurement only have year and month but not day
  mutate(year = lubridate::year(date)) %>% #extracting year to determine unique years later
  mutate(dec_date = decimal_date(date)) %>%
  group_by(site_id) %>% #by well...calculate:
  mutate(date_min = min(dec_date), 
         date_max = max(dec_date),
         measurement_dist = n_distinct(dtw), #distinct well measurements by well
         year_dist = n_distinct(year)) 

wy_all <- wy_nwis_clean %>% 
  select(agency_cd, site_id, date, dtw_ft = dtw, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>%
  mutate(source = "lb_national")

wy_unique_sites <- wy_nwis_clean %>% 
  select(agency_cd, site_id, date_min, date_max, measurement_dist, year_dist, lat_nad83 = lat, long_nad83 = long) %>% 
  mutate(source = "lb_national") %>% 
  distinct(site_id, .keep_all = TRUE)

```


```{r EXPORT}
#Write .csv files

write.csv(wy_unique_sites, here::here("National","CSV Export", "wy_unique_sites.csv"))

write.csv(wy_all, here::here("National", "CSV Export", "wy_all.csv"))

```


####**Wyoming TESTING** 
```{r WY TEST}

wy_sites <- readNWISdata(stateCd="Wyoming", 
                         service="site",
                         seriesCatalogOutput=TRUE) %>% #include this to include parm_cd & data_type_cd
    renameNWISColumns() %>% #at this step there are 393912 sites
 #filter(data_type_cd == "gw") %>% #down to 10748
  filter(site_tp_cd == "GW") %>% # down to 255838  (why SO different from data_type_cd = gw?)
  select(agency_cd, site_no, dec_lat_va, dec_long_va, site_tp_cd, data_type_cd, parm_cd) %>% 
distinct(site_no, .keep_all = TRUE) #%>%  #NOW only 12,217
  #drop_na(dec_long_va) #no lat long missing


# wy_sites_new <- whatNWISsites(stateCd="WY",
#                               seriesCatalogOutput=TRUE) %>% 
#   renameNWISColumns() %>% #at this step there are 22876 sites
#  filter(site_tp_cd == "GW") %>% #down to 18649 
#   distinct(site_no, .keep_all = TRUE) %>%  #stays at 18649
#   drop_na(dec_long_va) #no lat long missing



#wy_pcode <- c("00000","72019")
#Parm codes are 72019 and 00000 (a lot are 0000 which wouldn't have populated if I had origially pulled by param code)
  

wy_dtw <- readNWISdata(stateCd="Wyoming", 
                       service="gwlevels",
                       startDT="1850-01-01",
                       endDT="2020-04-22"
                       ) %>% 
  renameNWISColumns() %>% 
  select(agency_cd, site_no, lev_dt, lev_va)


#WY bbox: 40.982943,-111.194000,45.060851,-103.964996
#bBoxEx <- readNWISdata(bBox=c(-111.194000,40.982943,-103.964996,45.060851))
#Not effective only supplies 1,075 sites 

wy_nwis_join <- left_join(wy_dtw, wy_sites, by = "site_no") %>% 
  drop_na(lev_va) %>% 
  drop_na(dec_lat_va)

#length(unique(wy_nwis_join$site_no))
```
